# Story 2.6: AI engine runs entirely locally with open-source LLMs

## Status
Done

## Story
As a data artisan using the SQL Analyst application,
I want the AI engine to run entirely locally with open-source LLMs,
so that my data and queries never leave my machine, ensuring complete privacy and eliminating ongoing API costs while maintaining high-quality AI assistance.

## Acceptance Criteria

### Core Local LLM Integration
- [ ] System downloads and installs a selected open-source LLM (e.g., Llama 2, Code Llama, or Mistral)
- [ ] AI Core (Python) loads and manages the local LLM in memory
- [ ] Local LLM integrates seamlessly with existing Consensus Engine architecture
- [ ] System maintains same response quality as cloud-based models for SQL generation
- [ ] Local inference runs within acceptable performance parameters (<10 seconds per query)

### Security & Privacy
- [ ] All model downloads include cryptographic checksum verification
- [ ] Model files are extracted safely with path traversal protection
- [ ] No data or queries are transmitted to external services
- [ ] UI clearly indicates "Local Mode" status to reinforce privacy promise
- [ ] System handles model file permissions and access controls securely

### Performance & Resource Management
- [ ] System detects available hardware resources (RAM, GPU) and optimizes accordingly
- [ ] Model loading is optimized for startup time and memory usage
- [ ] System gracefully handles insufficient resources with clear user messaging
- [ ] Background model management doesn't interfere with other system operations
- [ ] Memory usage is monitored and managed to prevent system instability

### User Experience
- [ ] Zero-configuration experience - models work out of the box after installation
- [ ] Clear progress indicators during model download and initialization
- [ ] Fallback mechanisms if local model fails (with user consent)
- [ ] Settings panel allows users to select different local models
- [ ] System provides clear feedback about local vs. cloud operation mode

### Integration & Compatibility
- [ ] Local LLM integrates with existing correction learning system (Story 2.5)
- [ ] Project Cortex memory system works seamlessly with local models
- [ ] Consensus Engine can use multiple local models for cross-validation
- [ ] AI thought process visualization works with local model outputs
- [ ] All existing AI Core functionality remains intact

## Tasks / Subtasks

### Task 1: Model Selection and Integration Architecture
- [ ] Research and select optimal open-source LLM for SQL generation
- [ ] Design secure model download and verification system
- [ ] Create model management architecture within AI Core
- [ ] Define model switching and fallback mechanisms
- [ ] Document hardware requirements and recommendations

### Task 2: Secure Model Download and Installation System
- [ ] **MANDATORY SECURITY PRE-CHECK**: Run vulnerability scan on all model dependencies
- [ ] Implement cryptographic checksum verification for model downloads
- [ ] Create secure model extraction with path traversal protection
- [ ] Build progress tracking and error handling for downloads
- [ ] Implement model integrity validation and corruption detection

### Task 3: Local LLM Runtime Integration
- [ ] Integrate selected LLM library (e.g., llama.cpp, transformers, vLLM)
- [ ] Implement model loading and memory management
- [ ] Create inference pipeline compatible with Consensus Engine
- [ ] Optimize performance for available hardware (CPU/GPU detection)
- [ ] Implement proper model cleanup and resource deallocation

### Task 4: Consensus Engine Local Model Adaptation
- [ ] Modify Consensus Engine to work with local model inference
- [ ] Implement local model response parsing and validation
- [ ] Ensure correction learning system works with local models
- [ ] Maintain existing consensus scoring and validation logic
- [ ] Test multi-model consensus with multiple local models

### Task 5: UI/UX Local Mode Indicators
- [ ] Add "Local Mode" status indicator to main interface
- [ ] Create model selection and management settings panel
- [ ] Implement download progress and status displays
- [ ] Add privacy messaging to reinforce local operation
- [ ] Design fallback mode selection interface

### Task 6: Performance Optimization and Resource Management
- [ ] Implement hardware detection and optimization
- [ ] Create memory usage monitoring and management
- [ ] Optimize model loading and inference performance
- [ ] Implement background model management
- [ ] Add performance metrics and monitoring

### Task 7: Testing and Validation
- [ ] Test model download and installation process
- [ ] Validate inference quality against cloud models
- [ ] Performance testing across different hardware configurations
- [ ] Security testing of model handling and file operations
- [ ] Integration testing with all existing AI Core features

## Dev Notes

### Security Requirements (Zeus Directive Integration)
- **CRITICAL**: All model downloads MUST include SHA-256 checksum verification
- **CRITICAL**: Model extraction MUST use safe path handling to prevent directory traversal attacks
- **CRITICAL**: Run security vulnerability scan on all new dependencies before QA submission
- File permissions must be properly set for model files (read-only after installation)
- Model download URLs must be validated and use HTTPS only
- Implement secure temporary file handling during download/extraction

### Model Selection Criteria
- Prioritize models optimized for code/SQL generation (Code Llama, StarCoder, etc.)
- Consider model size vs. performance trade-offs for typical user hardware
- Evaluate licensing compatibility for commercial distribution
- Test inference speed and quality on representative SQL tasks
- Consider quantized models for better performance on consumer hardware

### Technical Architecture
- Use Python AI Core as the primary model management layer
- Implement model abstraction layer for easy model switching
- Consider using llama.cpp for efficient CPU inference
- Implement proper CUDA/GPU detection and utilization
- Design for future model updates and additions

### Performance Targets
- Model loading: <30 seconds on typical hardware
- Inference time: <10 seconds per query
- Memory usage: <8GB RAM for base configuration
- Startup impact: <10 seconds additional startup time
- Resource monitoring: Real-time memory and CPU tracking

### Integration Points
- Consensus Engine: Modify to accept local model responses
- Correction Learning: Ensure patterns work with local model outputs
- Memory System: Maintain compatibility with Project Cortex
- UI Components: Add local mode indicators and controls
- Error Handling: Graceful degradation when local models fail

### Installer Considerations
- Bundle selected model with application installer
- Implement delta updates for model improvements
- Consider optional model downloads for different use cases
- Ensure installer handles different OS and hardware configurations
- Plan for model licensing and attribution requirements

## QA Notes

### Security Focus Areas
- Verify all file operations use safe path handling
- Test model download integrity and checksum validation
- Validate that no data leaks to external services in local mode
- Confirm proper file permissions and access controls
- Test extraction process against malicious archives

### Performance Testing
- Benchmark inference speed across different hardware configurations
- Test memory usage under various load conditions
- Validate startup time impact and optimization
- Test concurrent operations and resource contention
- Measure quality degradation vs. cloud models

### Integration Testing
- Verify seamless operation with existing AI Core features
- Test correction learning with local model responses
- Validate consensus engine operation with local models
- Confirm UI properly reflects local vs. cloud operation
- Test fallback mechanisms and error recovery

## Definition of Done
- [ ] Selected open-source LLM runs locally within AI Core
- [ ] All model downloads include cryptographic verification
- [ ] UI clearly indicates local operation mode
- [ ] Performance meets defined targets (<10s inference, <8GB RAM)
- [ ] Security pre-check completed and vulnerabilities addressed
- [ ] Integration with existing AI features (consensus, learning, memory) works seamlessly
- [ ] Zero-configuration user experience achieved
- [ ] No external API calls made during local operation
- [ ] Comprehensive testing completed across security, performance, and integration
- [ ] Documentation updated for local model operation and troubleshooting
